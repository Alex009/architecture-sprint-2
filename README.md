# pymongo-api

## Как запустить

Запускаем mongodb и приложение

```shell
docker compose up -d
```

Заполняем mongodb данными

```shell
./scripts/mongo-init.sh
```

## Как проверить

### Если вы запускаете проект на локальной машине

Откройте в браузере http://localhost:8080

### Если вы запускаете проект на предоставленной виртуальной машине

Узнать белый ip виртуальной машины

```shell
curl --silent http://ifconfig.me
```

Откройте в браузере http://<ip виртуальной машины>:8080

## Доступные эндпоинты

Список доступных эндпоинтов, swagger http://<ip виртуальной машины>:8080/docs

## Пояснения к решению

### Кластер MongoDB

Единственный инстанс MongoDB был заменен на кластер с двумя шардами, у каждого шарда по 2 реплики (primary, secondary, secondary - 3 инстанса на шард). Так как в процессе распродаж будет активно использоваться функционал создания заказа и изменения остатков, то нам важно иметь актуальные данные, а значит приоритет при чтении будет с первичной ноды.

Для работы кластера также добавлены роутер (для определения какой запрос с помощью какого шарда будет обрабатываться) и сервер конфигурации (для настройки и автоматической перебалансировки при отказах каких либо инстансов).

Роутер и сервер конфигурации также имеют по 2 реплики, для повышения отказоустойчивости.

MongoDB рекомендует использовать не менее 3 инстансов, так как используется алгоритм консеснуса при отказах первичной ноды. Если будет первичная и вторичная, то вероятна ситуация при которой после технических проблем станет две мастер ноды. Поэтому все ноды что мы реплицируем имеют по 2 реплики.

Клиентское приложение будет работать автоматически с одним из роутеров, а с каким - будет решать драйвер mongodb из клиентской библиотеки mongodb. В строке подключения мы укажем все наши роутеры и драйвер будет переключаться между ними при отказах.

При подключении остальных нод к конфигурационному ReplicaSet будем указывать все ноды конфигурационных серверов, таким образом каждая нода обоих шардов и каждая нода роутера будет знать о всех 3 нодах конфигурационного сервера.

Роутер и конфигурационный сервер решено было реплицировать так как повышенная нагрузка распродаж слабо прогнозируется, мы не можем точно знать насколько высокая нагрузка окажется, а роутеры могут стать узким местом. Конфигурационный сервер менее вероятно что станет узким местом, но будет очень не к месту если одна единственная нода конфигурационного сервера ляжет как раз во время распродажи. Конфигурационный сервер не требует сильно много ресурсов, поэтому его репликация не является сильно затратной. А роутер однозначно будет получать нагрузку, так как является входной точкой для всех запросов к бд.

Также роутеры не используют алгоритм консенсуса, поэтому там мы можем при желании использовать и 2 ноды. Однако учитывая что именно роутеры являются входной точкой - принято решение сделать некий запас, поэтому 3 инстанса роутеров.

### Кластер Redis

Так как у нас онлайн магазин и нужно обеспечить отказоустойчивость во время распродаж, то решено сделать упор на кешировании всех статичных данных - сам каталог товаров статичен, поэтому мы можем использовать кеширование чтобы значительно снизить нагрузку на базу данных, оставив её на обработке заказов и остатков.

Так как каталог товаров у магазина предполагается что достаточно обширный и помимо каталога мы можем закешировать также профиль пользователя и  рекомендации, а также нам крайне важно недопустить резкого наплыва запросов в базу данных - нам важно обеспечить отказоустойчивость кеша. Если вдруг кеш сервер станет недоступен, то все запросы пользователей сразу польются в базу данных, что вполне вероятно создаст заметные задержки, хотя и не должно полностью остановить работу, ведь база данных у нас уже в кластере.

Чтобы обеспечить отказоустойчивость кеша используем рекомендуемую минимальную схему для Redis - 3 мастер ноды и по 1 реплике к каждой, то есть суммарно 6 нод.

Клиентское приложение будет иметь доступ ко всем 6 нодам кеша и при подключении к redis будет использовать специальную строку подключения, где перечислены все ноды redis, для автоматического переключения между нодами в случаях отказов и задержек.

### Мониторинг

Помимо добавления шардирования и реплицирования базы данных, а также добавления кеширования, нужно добавить систему мониторинга и отправки оповещений, чтобы в случаях отказов частей системы успеть среагировать, так как распродажи создают большую нагрузку в короткое время.

Система должна успешно выдержать отказ нескольких нод от базы данных и от кеша, но если не предпринимать опреативных действий то количество отказов может достигнуть критического - например из 3 роутеров бд останется 1 и создаст такие задержки запросов, из-за которых создание заказов перестанет успешно выполняться. Либо упадет один из шардов бд, после чего часть данных станут недоступны и пользователи посчитают что часть информации потеряна (например пропадет часть каталога, либо часть заказов).

Поэтому планируется добавить мониторинг в виде Prometheus с настроенными алертами и AlarmManager, а также Graphana для наблюдения за показателями в критический период во время распродаж.

Для передачи данных от redis кластера поднимем 1 инстанс redis-exporter, который будет наблюдать за всеми 6 нодами кластера и передавать данные в Prometheus. Также добавим алерт и на доступность самого redis-exporter на случай его отказа. Также будут настроены и алерты на показатели самих redis нод - доступность, нагрузка на цп, память, количество запросов, задержки запросов, cache hitrate. Потеря мониторинга redis, как и потеря самого кеша redis, не станет критическим отказом, поэтому мы снижаем затраты и сложность реализации в данном месте.

Для передачи данных от mongodb кластера поднимем 3 инстанса mongodb-exporter, они будут следить за отдельными частями - один за первым шардом, второй за другим и третий за роутерами и конфиг серверами. Все данные будут передаваться также в Prometheus и тоже будут настроены алерты на доступность mongodb-exporter'ов. Также будут настроены и алерты на показатели самих mongodb нод - доступность, нагрузка на цп, память, количество запросов, задержки запросов.



