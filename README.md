# pymongo-api

## Как запустить

Запускаем mongodb и приложение

```shell
docker compose up -d
```

Заполняем mongodb данными

```shell
./scripts/mongo-init.sh
```

## Как проверить

### Если вы запускаете проект на локальной машине

Откройте в браузере http://localhost:8080

### Если вы запускаете проект на предоставленной виртуальной машине

Узнать белый ip виртуальной машины

```shell
curl --silent http://ifconfig.me
```

Откройте в браузере http://<ip виртуальной машины>:8080

## Доступные эндпоинты

Список доступных эндпоинтов, swagger http://<ip виртуальной машины>:8080/docs

## Пояснения к решению

### Кластер MongoDB

Единственный инстанс MongoDB был заменен на кластер с двумя шардами, у каждого шарда по 2 реплики (primary, secondary, secondary - 3 инстанса на шард). Так как в процессе распродаж будет активно использоваться функционал создания заказа и изменения остатков, то нам важно иметь актуальные данные, а значит приоритет при чтении будет с первичной ноды.

Для работы кластера также добавлены роутер (для определения какой запрос с помощью какого шарда будет обрабатываться) и сервер конфигурации (для настройки и автоматической перебалансировки при отказах каких либо инстансов).

Роутер и сервер конфигурации также имеют по 2 реплики, для повышения отказоустойчивости.

MongoDB рекомендует использовать не менее 3 инстансов, так как используется алгоритм консеснуса при отказах первичной ноды. Если будет первичная и вторичная, то вероятна ситуация при которой после технических проблем станет две мастер ноды. Поэтому все ноды что мы реплицируем имеют по 2 реплики.

Клиентское приложение будет работать автоматически с одним из роутеров, а с каким - будет решать драйвер mongodb из клиентской библиотеки mongodb. В строке подключения мы укажем все наши роутеры и драйвер будет переключаться между ними при отказах.

При подключении остальных нод к конфигурационному ReplicaSet будем указывать все ноды конфигурационных серверов, таким образом каждая нода обоих шардов и каждая нода роутера будет знать о всех 3 нодах конфигурационного сервера.

Роутер и конфигурационный сервер решено было реплицировать так как повышенная нагрузка распродаж слабо прогнозируется, мы не можем точно знать насколько высокая нагрузка окажется, а роутеры могут стать узким местом. Конфигурационный сервер менее вероятно что станет узким местом, но будет очень не к месту если одна единственная нода конфигурационного сервера ляжет как раз во время распродажи. Конфигурационный сервер не требует сильно много ресурсов, поэтому его репликация не является сильно затратной. А роутер однозначно будет получать нагрузку, так как является входной точкой для всех запросов к бд.

Также роутеры не используют алгоритм консенсуса, поэтому там мы можем при желании использовать и 2 ноды. Однако учитывая что именно роутеры являются входной точкой - принято решение сделать некий запас, поэтому 3 инстанса роутеров.

### Кластер Redis

Так как у нас онлайн магазин и нужно обеспечить отказоустойчивость во время распродаж, то решено сделать упор на кешировании всех статичных данных - сам каталог товаров статичен, поэтому мы можем использовать кеширование чтобы значительно снизить нагрузку на базу данных, оставив её на обработке заказов и остатков.

Так как каталог товаров у магазина предполагается что достаточно обширный и помимо каталога мы можем закешировать также профиль пользователя и  рекомендации, а также нам крайне важно недопустить резкого наплыва запросов в базу данных - нам важно обеспечить отказоустойчивость кеша. Если вдруг кеш сервер станет недоступен, то все запросы пользователей сразу польются в базу данных, что вполне вероятно создаст заметные задержки, хотя и не должно полностью остановить работу, ведь база данных у нас уже в кластере.

Чтобы обеспечить отказоустойчивость кеша используем рекомендуемую минимальную схему для Redis - 3 мастер ноды и по 1 реплике к каждой, то есть суммарно 6 нод.

Клиентское приложение будет иметь доступ ко всем 6 нодам кеша и при подключении к redis будет использовать специальную строку подключения, где перечислены все ноды redis, для автоматического переключения между нодами в случаях отказов и задержек.




